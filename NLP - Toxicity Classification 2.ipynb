{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8733d079",
   "metadata": {},
   "source": [
    "# 1| Importa√ß√£o das bibliotecas e ajuste do notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4510e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad0f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b660510",
   "metadata": {},
   "source": [
    "# 2 | Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dced521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANDO O DATASET \n",
    "\n",
    "df_raw = pd.read_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - An√°lise de Toxidade\\\\train (2).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e66eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIANDO UM BACKUP\n",
    "\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334db9b",
   "metadata": {},
   "source": [
    "# 3| Entendimento dos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abca414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset possui 16800 linhas e 2 colunas\n"
     ]
    }
   ],
   "source": [
    "# DIMENS√ïES DO DATASET\n",
    "\n",
    "print(\"O dataset possui\",df.shape[0],\"linhas e\",df.shape[1],\"colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc6909b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @user olha quem chegouuuuu, nossos queridin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>veio umas teorias muito loucas na minha cabe√ßa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user üòÇüòÇüòÇüòÇmais nao tinha falado ontem qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @user quer ser filha da puta logo comigo qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vai besta üòÇüòÇüòÇüòÇ casquei com a ultima foto</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  rt @user olha quem chegouuuuu, nossos queridin...      0\n",
       "1  veio umas teorias muito loucas na minha cabe√ßa...      1\n",
       "2  @user @user üòÇüòÇüòÇüòÇmais nao tinha falado ontem qu...      0\n",
       "3  rt @user quer ser filha da puta logo comigo qu...      1\n",
       "4           vai besta üòÇüòÇüòÇüòÇ casquei com a ultima foto      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PR√â-VISUALIZA√á√ÉO DOS DADOS\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b253a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9425</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7375</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count     %\n",
       "label             \n",
       "0       9425  56.0\n",
       "1       7375  44.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AN√ÅLISE DA PROPOR√á√ÉO DE CADA UMA DAS CLASSES\n",
    "\n",
    "proporcao = df['label'].value_counts().to_frame()\n",
    "\n",
    "proporcao['%'] = (proporcao['count'] / df.shape[0]) * 100\n",
    "\n",
    "proporcao['%'] = proporcao['%'].round()\n",
    "\n",
    "proporcao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8330efa",
   "metadata": {},
   "source": [
    "# 4| Fun√ß√µes de limpeza de dados e Pipeline para limpar os dados automaticamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0548ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CRIANDO ALGUMAS FUN√á√ïES PARA FACILITAR A LIMPEZA DE PARTE DOS DADOS\n",
    "\n",
    "# retirar os links \n",
    "def remove_https(text):\n",
    "    return re.sub(r'https\\S+', '', text)\n",
    "\n",
    "# remover emojis\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  \n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                               u\"\\U00002500-\\U00002BEF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  \n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Definindo as stopwords em ptbr\n",
    "stop_words = set(stopwords.words('portuguese'))  \n",
    "stop_words.add('rt')\n",
    "stop_words.add('@user')\n",
    "stop_words.add('\\n')\n",
    "\n",
    "# retirar stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#### CRIANDO UMA FUN√á√ÉO DE PRE-PROCESSAMENTO PARA APLICAR TODAS DE UMA VEZ e RETORNAR O DADO LIMPO\n",
    "\n",
    "def pipeline(text):\n",
    "    # deixar todos os dados min√∫sculos\n",
    "    text = text.lower()\n",
    "    \n",
    "    # retirar pontua√ß√µes e sinais\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i, \"\")\n",
    "        \n",
    "    # retirar os links \n",
    "    text = remove_https(text)\n",
    "    \n",
    "    # retirar emojis\n",
    "    text = remove_emojis(text) # sem emojis\n",
    "    \n",
    "    # retirar acentos\n",
    "    text = unidecode.unidecode(text)\n",
    "        \n",
    "    # retirar stopwords\n",
    "    text = remove_stopwords(text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02616020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma coluna para armazenar os dados tratados/limpos\n",
    "df['text_adjusted'] = df['text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "257508b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando os tratamentos na coluna criada\n",
    "df['text_adjusted'] = df['text_adjusted'].apply(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dadc6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @user olha quem chegouuuuu, nossos queridin...</td>\n",
       "      <td>0</td>\n",
       "      <td>user olha chegouuuuu queridinhos vem direcao f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>veio umas teorias muito loucas na minha cabe√ßa...</td>\n",
       "      <td>1</td>\n",
       "      <td>veio umas teorias loucas cabeca agora pqp to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user üòÇüòÇüòÇüòÇmais nao tinha falado ontem qu...</td>\n",
       "      <td>0</td>\n",
       "      <td>user user nao falado ontem nao ia patrocinado ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @user quer ser filha da puta logo comigo qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>user quer filha puta logo comigo 50x pior kkkk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vai besta üòÇüòÇüòÇüòÇ casquei com a ultima foto</td>\n",
       "      <td>1</td>\n",
       "      <td>vai besta casquei ultima foto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>performer da na√ß√£o caralho</td>\n",
       "      <td>1</td>\n",
       "      <td>performer nacao caralho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16796</th>\n",
       "      <td>v√¥lei feminino √© foda n√©, pqp, s√≥ vem t√≥quio</td>\n",
       "      <td>1</td>\n",
       "      <td>volei feminino foda ne pqp so vem toquio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16797</th>\n",
       "      <td>@user cara de pau, quem desrespeita a constitu...</td>\n",
       "      <td>1</td>\n",
       "      <td>user cara pau desrespeita constituicao federal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16798</th>\n",
       "      <td>duas das grandes atletas do frescobol mundial....</td>\n",
       "      <td>0</td>\n",
       "      <td>duas grandes atletas frescobol mundial academi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16799</th>\n",
       "      <td>fui comprar um, sair com 7 brincos kkkkkk pqp</td>\n",
       "      <td>0</td>\n",
       "      <td>comprar sair 7 brincos kkkkkk pqp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16800 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      rt @user olha quem chegouuuuu, nossos queridin...      0   \n",
       "1      veio umas teorias muito loucas na minha cabe√ßa...      1   \n",
       "2      @user @user üòÇüòÇüòÇüòÇmais nao tinha falado ontem qu...      0   \n",
       "3      rt @user quer ser filha da puta logo comigo qu...      1   \n",
       "4               vai besta üòÇüòÇüòÇüòÇ casquei com a ultima foto      1   \n",
       "...                                                  ...    ...   \n",
       "16795                         performer da na√ß√£o caralho      1   \n",
       "16796       v√¥lei feminino √© foda n√©, pqp, s√≥ vem t√≥quio      1   \n",
       "16797  @user cara de pau, quem desrespeita a constitu...      1   \n",
       "16798  duas das grandes atletas do frescobol mundial....      0   \n",
       "16799      fui comprar um, sair com 7 brincos kkkkkk pqp      0   \n",
       "\n",
       "                                           text_adjusted  \n",
       "0      user olha chegouuuuu queridinhos vem direcao f...  \n",
       "1      veio umas teorias loucas cabeca agora pqp to a...  \n",
       "2      user user nao falado ontem nao ia patrocinado ...  \n",
       "3      user quer filha puta logo comigo 50x pior kkkk...  \n",
       "4                          vai besta casquei ultima foto  \n",
       "...                                                  ...  \n",
       "16795                            performer nacao caralho  \n",
       "16796           volei feminino foda ne pqp so vem toquio  \n",
       "16797  user cara pau desrespeita constituicao federal...  \n",
       "16798  duas grandes atletas frescobol mundial academi...  \n",
       "16799                  comprar sair 7 brincos kkkkkk pqp  \n",
       "\n",
       "[16800 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza√ß√£o do resultado\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0890aba8",
   "metadata": {},
   "source": [
    "# 5| Separa√ß√£o de dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6732523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um novo dataframe a ser usado no treinamento apenas com os dados tratados + label\n",
    "df_modeling = df[['text_adjusted','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fae1e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando em vari√°vel target e preditora\n",
    "X = df['text_adjusted']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "682bada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os dados de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9d8b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustando o tokenizer nos dados de treino\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Aplicando o tokenizer aos dados de treino e teste\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Quantidade de palavras (ser√° usada na camada de embedding)\n",
    "qtd_palavras = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acfff238",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34a568fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hot_labels (idk whty tapi ini penting, kalo ga bakal error)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "y_train = to_categorical(y_train, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed947774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test tweet: 3360\n",
      "num train tweet: 13440\n"
     ]
    }
   ],
   "source": [
    "# another look on the number of tweet in test and training data\n",
    "\n",
    "print(f\"num test tweet: {y_test.shape[0]}\")\n",
    "print(f\"num train tweet: {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39ba957",
   "metadata": {},
   "source": [
    "# 6| Cria√ß√£o do modelo 1 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5949f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+tf.keras.backend.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6232c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dis if u want\n",
    "output_dim = 200\n",
    "\n",
    "# LSTM model architechture (CNN + LSTM)\n",
    "model = Sequential([\n",
    "    # embedding layer is like idk\n",
    "    Embedding(qtd_palavras, output_dim,input_shape=(max_length,)),\n",
    "    # lstm for xxx\n",
    "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # dense to connect the previous output with current layer\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # this is output layer, with 3 class (0, 1, 2)\n",
    "    Dense(2, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcc702a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ<span style=\"font-weight: bold\"> Layer (type)                         </span>‚îÉ<span style=\"font-weight: bold\"> Output Shape                </span>‚îÉ<span style=\"font-weight: bold\">         Param # </span>‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)             ‚îÇ       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104,400</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  ‚îÇ          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,840</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 ‚îÇ           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 ‚îÇ               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      ‚îÇ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   ‚îÇ             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
       "‚îÉ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
       "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
       "‚îÇ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m200\u001b[0m)             ‚îÇ       \u001b[38;5;34m4,104,400\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  ‚îÇ          \u001b[38;5;34m67,840\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout (\u001b[38;5;33mDropout\u001b[0m)                    ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense (\u001b[38;5;33mDense\u001b[0m)                        ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 ‚îÇ           \u001b[38;5;34m8,320\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 ‚îÇ               \u001b[38;5;34m0\u001b[0m ‚îÇ\n",
       "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
       "‚îÇ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      ‚îÇ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   ‚îÇ             \u001b[38;5;34m258\u001b[0m ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,180,818</span> (15.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,180,818\u001b[0m (15.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,180,818</span> (15.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,180,818\u001b[0m (15.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b56e5f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - accuracy: 0.5934 - f1: 0.5934 - loss: 0.6589 - precision: 0.5934 - recall: 0.5934 - val_accuracy: 0.7491 - val_f1: 0.7500 - val_loss: 0.5141 - val_precision: 0.7500 - val_recall: 0.7500\n",
      "Epoch 2/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.8113 - f1: 0.8113 - loss: 0.4240 - precision: 0.8113 - recall: 0.8113 - val_accuracy: 0.7446 - val_f1: 0.7447 - val_loss: 0.5336 - val_precision: 0.7447 - val_recall: 0.7447\n",
      "Epoch 3/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.8766 - f1: 0.8766 - loss: 0.2869 - precision: 0.8766 - recall: 0.8766 - val_accuracy: 0.7268 - val_f1: 0.7267 - val_loss: 0.6343 - val_precision: 0.7267 - val_recall: 0.7267\n",
      "Epoch 4/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - accuracy: 0.9107 - f1: 0.9107 - loss: 0.2150 - precision: 0.9107 - recall: 0.9107 - val_accuracy: 0.7188 - val_f1: 0.7190 - val_loss: 0.7801 - val_precision: 0.7190 - val_recall: 0.7190\n",
      "Epoch 5/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - accuracy: 0.9346 - f1: 0.9346 - loss: 0.1639 - precision: 0.9346 - recall: 0.9346 - val_accuracy: 0.7033 - val_f1: 0.7031 - val_loss: 0.8706 - val_precision: 0.7031 - val_recall: 0.7031\n",
      "Epoch 6/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.9435 - f1: 0.9435 - loss: 0.1341 - precision: 0.9435 - recall: 0.9435 - val_accuracy: 0.6973 - val_f1: 0.6972 - val_loss: 1.0506 - val_precision: 0.6972 - val_recall: 0.6972\n",
      "Epoch 7/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.9453 - f1: 0.9453 - loss: 0.1218 - precision: 0.9453 - recall: 0.9453 - val_accuracy: 0.6949 - val_f1: 0.6952 - val_loss: 1.3158 - val_precision: 0.6952 - val_recall: 0.6952\n",
      "Epoch 8/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.9558 - f1: 0.9558 - loss: 0.0988 - precision: 0.9558 - recall: 0.9558 - val_accuracy: 0.6917 - val_f1: 0.6919 - val_loss: 1.5341 - val_precision: 0.6919 - val_recall: 0.6919\n",
      "Epoch 9/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - accuracy: 0.9564 - f1: 0.9564 - loss: 0.0901 - precision: 0.9564 - recall: 0.9564 - val_accuracy: 0.6821 - val_f1: 0.6828 - val_loss: 1.6843 - val_precision: 0.6828 - val_recall: 0.6828\n",
      "Epoch 10/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.9593 - f1: 0.9593 - loss: 0.0877 - precision: 0.9593 - recall: 0.9593 - val_accuracy: 0.6926 - val_f1: 0.6928 - val_loss: 1.5475 - val_precision: 0.6928 - val_recall: 0.6928\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e67117",
   "metadata": {},
   "source": [
    "# 6.1| Predi√ß√£o Competi√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c443aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download do conjunto de teste a ser predito na competi√ß√£o\n",
    "df_teste = pd.read_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - An√°lise de Toxidade\\\\test (4).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "000e1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o pipeline de tratamento de dados\n",
    "df_teste['text'] = df_teste['text'].apply(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd2cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a coluna com os textos a serem classificados em 'array'\n",
    "X_pred = df_teste['text']\n",
    "\n",
    "# Tokenizando os textos\n",
    "X_pred = tokenizer.texts_to_sequences(X_pred)\n",
    "\n",
    "X_pred = pad_sequences(X_pred, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c18afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predi√ß√£o\n",
    "y_pred = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "644e7822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9126071e-01, 8.7393140e-03],\n",
       "       [9.9899036e-01, 1.0096114e-03],\n",
       "       [5.8373843e-07, 9.9999940e-01],\n",
       "       ...,\n",
       "       [1.0000000e+00, 5.7883054e-09],\n",
       "       [3.6541006e-01, 6.3458997e-01],\n",
       "       [9.9125779e-01, 8.7422552e-03]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualiza√ß√£o das classifica√ß√µes feitas\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2397df",
   "metadata": {},
   "source": [
    "> Nota-se que a predi√ß√£o √© a probabilidade do texto avaliado ser classificado para cada classe (coluna 0 = n√£o t√≥xico e coluna 1 = t√≥xico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94b784ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiar para decidir a classe\n",
    "threshold = 0.7\n",
    "\n",
    "# Convertendo as probabilidades para as classes finais usando um limiar\n",
    "predictions = np.where(y_pred >= threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce4429c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo as classifica√ß√µes em uma array\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f8ae309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando as classifica√ß√µes em uma nova coluna\n",
    "df_teste['label'] = predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4221d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Excluindo a coluna de 'text' para manter o formato de submiss√£o da competi√ß√£o no Kaggle\n",
    "df_teste.drop('text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed37740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o df_teste em um csv para ser submetido ao Kaggle\n",
    "df_teste.to_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - An√°lise de Toxidade\\\\Submiss√µes\\\\submissao_lstm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747594f",
   "metadata": {},
   "source": [
    "> O modelo de LSTM obteve um resultado de xxx de acur√°cia nos dados da competi√ß√£o. Com isso, outros modelos ser√£o tamb√©m treinados e testados para tentar alcan√ßar a melhor pontua√ß√£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a5fb8",
   "metadata": {},
   "source": [
    "# 7 | Modelo SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f84dbdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia do modelo: 0.743452380952381\n"
     ]
    }
   ],
   "source": [
    "# Divis√£o dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_modeling['text_adjusted'], df_modeling['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Vetoriza√ß√£o do texto usando TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Treinamento do modelo SVM\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train_vect, y_train)\n",
    "\n",
    "# Fazendo previs√µes\n",
    "predictions = model.predict(X_test_vect)\n",
    "\n",
    "# Avaliando o modelo\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Acur√°cia do modelo:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c842b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_modeling['text_adjusted'], df_modeling['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Instanciando o tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustando o tokenizer nos dados de treino\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Aplicando o tokenizer aos dados de treino e teste\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Quantidade de palavras (ser√° usada na camada de embedding)\n",
    "qtd_palavras = len(tokenizer.word_index) + 1\n",
    "\n",
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)\n",
    "\n",
    "# create hot_labels (idk whty tapi ini penting, kalo ga bakal error)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "\n",
    "# Convertendo as classifica√ß√µes em uma array\n",
    "y_test_array = np.argmax(y_test, axis=1)\n",
    "y_train_array = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Treinamento do modelo SVM\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train_array)\n",
    "\n",
    "# Fazendo previs√µes\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Avaliando o modelo\n",
    "accuracy = accuracy_score(y_test_array, predictions)\n",
    "print(\"Acur√°cia do modelo:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4da026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo as classifica√ß√µes em uma array\n",
    "teste = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
