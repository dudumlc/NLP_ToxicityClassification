{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8733d079",
   "metadata": {},
   "source": [
    "# 1| ImportaÃ§Ã£o das bibliotecas e ajuste do notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed4510e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ad0f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import unidecode\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b660510",
   "metadata": {},
   "source": [
    "# 2 | Leitura dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dced521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANDO O DATASET \n",
    "\n",
    "df_raw = pd.read_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - AnÃ¡lise de Toxidade\\\\train (2).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e66eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIANDO UM BACKUP\n",
    "\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334db9b",
   "metadata": {},
   "source": [
    "# 3| Entendimento dos dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abca414a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset possui 16800 linhas e 2 colunas\n"
     ]
    }
   ],
   "source": [
    "# DIMENSÃ•ES DO DATASET\n",
    "\n",
    "print(\"O dataset possui\",df.shape[0],\"linhas e\",df.shape[1],\"colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc6909b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @user olha quem chegouuuuu, nossos queridin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>veio umas teorias muito loucas na minha cabeÃ§a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚mais nao tinha falado ontem qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @user quer ser filha da puta logo comigo qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vai besta ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ casquei com a ultima foto</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  rt @user olha quem chegouuuuu, nossos queridin...      0\n",
       "1  veio umas teorias muito loucas na minha cabeÃ§a...      1\n",
       "2  @user @user ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚mais nao tinha falado ontem qu...      0\n",
       "3  rt @user quer ser filha da puta logo comigo qu...      1\n",
       "4           vai besta ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ casquei com a ultima foto      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PRÃ‰-VISUALIZAÃ‡ÃƒO DOS DADOS\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b253a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9425</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7375</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count     %\n",
       "label             \n",
       "0       9425  56.0\n",
       "1       7375  44.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANÃLISE DA PROPORÃ‡ÃƒO DE CADA UMA DAS CLASSES\n",
    "\n",
    "proporcao = df['label'].value_counts().to_frame()\n",
    "\n",
    "proporcao['%'] = (proporcao['count'] / df.shape[0]) * 100\n",
    "\n",
    "proporcao['%'] = proporcao['%'].round()\n",
    "\n",
    "proporcao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8330efa",
   "metadata": {},
   "source": [
    "# 4| FunÃ§Ãµes de limpeza de dados e Pipeline para limpar os dados automaticamente "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0548ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CRIANDO ALGUMAS FUNÃ‡Ã•ES PARA FACILITAR A LIMPEZA DE PARTE DOS DADOS\n",
    "\n",
    "# retirar os links \n",
    "def remove_https(text):\n",
    "    return re.sub(r'https\\S+', '', text)\n",
    "\n",
    "# remover emojis\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  \n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                               u\"\\U00002500-\\U00002BEF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  \n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Definindo as stopwords em ptbr\n",
    "stop_words = set(stopwords.words('portuguese'))  \n",
    "stop_words.add('rt')\n",
    "stop_words.add('@user')\n",
    "stop_words.add('\\n')\n",
    "\n",
    "# retirar stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "#### CRIANDO UMA FUNÃ‡ÃƒO DE PRE-PROCESSAMENTO PARA APLICAR TODAS DE UMA VEZ e RETORNAR O DADO LIMPO\n",
    "\n",
    "def pipeline(text):\n",
    "    # deixar todos os dados minÃºsculos\n",
    "    text = text.lower()\n",
    "    \n",
    "    # retirar pontuaÃ§Ãµes e sinais\n",
    "    for i in string.punctuation:\n",
    "        text = text.replace(i, \"\")\n",
    "        \n",
    "    # retirar os links \n",
    "    text = remove_https(text)\n",
    "    \n",
    "    # retirar emojis\n",
    "    text = remove_emojis(text) # sem emojis\n",
    "    \n",
    "    # retirar acentos\n",
    "    text = unidecode.unidecode(text)\n",
    "        \n",
    "    # retirar stopwords\n",
    "    text = remove_stopwords(text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02616020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma coluna para armazenar os dados tratados/limpos\n",
    "df['text_adjusted'] = df['text'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "257508b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando os tratamentos na coluna criada\n",
    "df['text_adjusted'] = df['text_adjusted'].apply(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dadc6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_adjusted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @user olha quem chegouuuuu, nossos queridin...</td>\n",
       "      <td>0</td>\n",
       "      <td>user olha chegouuuuu queridinhos vem direcao f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>veio umas teorias muito loucas na minha cabeÃ§a...</td>\n",
       "      <td>1</td>\n",
       "      <td>veio umas teorias loucas cabeca agora pqp to a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚mais nao tinha falado ontem qu...</td>\n",
       "      <td>0</td>\n",
       "      <td>user user nao falado ontem nao ia patrocinado ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @user quer ser filha da puta logo comigo qu...</td>\n",
       "      <td>1</td>\n",
       "      <td>user quer filha puta logo comigo 50x pior kkkk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vai besta ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ casquei com a ultima foto</td>\n",
       "      <td>1</td>\n",
       "      <td>vai besta casquei ultima foto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16795</th>\n",
       "      <td>performer da naÃ§Ã£o caralho</td>\n",
       "      <td>1</td>\n",
       "      <td>performer nacao caralho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16796</th>\n",
       "      <td>vÃ´lei feminino Ã© foda nÃ©, pqp, sÃ³ vem tÃ³quio</td>\n",
       "      <td>1</td>\n",
       "      <td>volei feminino foda ne pqp so vem toquio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16797</th>\n",
       "      <td>@user cara de pau, quem desrespeita a constitu...</td>\n",
       "      <td>1</td>\n",
       "      <td>user cara pau desrespeita constituicao federal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16798</th>\n",
       "      <td>duas das grandes atletas do frescobol mundial....</td>\n",
       "      <td>0</td>\n",
       "      <td>duas grandes atletas frescobol mundial academi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16799</th>\n",
       "      <td>fui comprar um, sair com 7 brincos kkkkkk pqp</td>\n",
       "      <td>0</td>\n",
       "      <td>comprar sair 7 brincos kkkkkk pqp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16800 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label  \\\n",
       "0      rt @user olha quem chegouuuuu, nossos queridin...      0   \n",
       "1      veio umas teorias muito loucas na minha cabeÃ§a...      1   \n",
       "2      @user @user ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚mais nao tinha falado ontem qu...      0   \n",
       "3      rt @user quer ser filha da puta logo comigo qu...      1   \n",
       "4               vai besta ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ casquei com a ultima foto      1   \n",
       "...                                                  ...    ...   \n",
       "16795                         performer da naÃ§Ã£o caralho      1   \n",
       "16796       vÃ´lei feminino Ã© foda nÃ©, pqp, sÃ³ vem tÃ³quio      1   \n",
       "16797  @user cara de pau, quem desrespeita a constitu...      1   \n",
       "16798  duas das grandes atletas do frescobol mundial....      0   \n",
       "16799      fui comprar um, sair com 7 brincos kkkkkk pqp      0   \n",
       "\n",
       "                                           text_adjusted  \n",
       "0      user olha chegouuuuu queridinhos vem direcao f...  \n",
       "1      veio umas teorias loucas cabeca agora pqp to a...  \n",
       "2      user user nao falado ontem nao ia patrocinado ...  \n",
       "3      user quer filha puta logo comigo 50x pior kkkk...  \n",
       "4                          vai besta casquei ultima foto  \n",
       "...                                                  ...  \n",
       "16795                            performer nacao caralho  \n",
       "16796           volei feminino foda ne pqp so vem toquio  \n",
       "16797  user cara pau desrespeita constituicao federal...  \n",
       "16798  duas grandes atletas frescobol mundial academi...  \n",
       "16799                  comprar sair 7 brincos kkkkkk pqp  \n",
       "\n",
       "[16800 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VisualizaÃ§Ã£o do resultado\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0890aba8",
   "metadata": {},
   "source": [
    "# 5| SeparaÃ§Ã£o de dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6732523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um novo dataframe a ser usado no treinamento apenas com os dados tratados + label\n",
    "df_modeling = df[['text_adjusted','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fae1e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando em variÃ¡vel target e preditora\n",
    "X = df['text_adjusted']\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "682bada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os dados de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9d8b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustando o tokenizer nos dados de treino\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Aplicando o tokenizer aos dados de treino e teste\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Quantidade de palavras (serÃ¡ usada na camada de embedding)\n",
    "qtd_palavras = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acfff238",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34a568fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hot_labels (idk whty tapi ini penting, kalo ga bakal error)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "y_train = to_categorical(y_train, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed947774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num test tweet: 3360\n",
      "num train tweet: 13440\n"
     ]
    }
   ],
   "source": [
    "# another look on the number of tweet in test and training data\n",
    "\n",
    "print(f\"num test tweet: {y_test.shape[0]}\")\n",
    "print(f\"num train tweet: {y_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39ba957",
   "metadata": {},
   "source": [
    "# 6| CriaÃ§Ã£o do modelo 1 - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5949f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall(y_true, y_pred):\n",
    "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    precisions = precision(y_true, y_pred)\n",
    "    recalls = recall(y_true, y_pred)\n",
    "    return 2*((precisions*recalls)/(precisions+recalls+tf.keras.backend.epsilon()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6232c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change dis if u want\n",
    "output_dim = 200\n",
    "\n",
    "# LSTM model architechture (CNN + LSTM)\n",
    "model = Sequential([\n",
    "    # embedding layer is like idk\n",
    "    Embedding(qtd_palavras, output_dim,input_shape=(max_length,)),\n",
    "    # lstm for xxx\n",
    "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # dense to connect the previous output with current layer\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    # dropout to prevent overfitting\n",
    "    Dropout(0.5),\n",
    "    # this is output layer, with 3 class (0, 1, 2)\n",
    "    Dense(2, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcc702a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">55</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)             â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,104,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,840</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m55\u001b[0m, \u001b[38;5;34m200\u001b[0m)             â”‚       \u001b[38;5;34m4,104,400\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚          \u001b[38;5;34m67,840\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   â”‚             \u001b[38;5;34m258\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,180,818</span> (15.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,180,818\u001b[0m (15.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,180,818</span> (15.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,180,818\u001b[0m (15.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b56e5f1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 59ms/step - accuracy: 0.5934 - f1: 0.5934 - loss: 0.6589 - precision: 0.5934 - recall: 0.5934 - val_accuracy: 0.7491 - val_f1: 0.7500 - val_loss: 0.5141 - val_precision: 0.7500 - val_recall: 0.7500\n",
      "Epoch 2/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.8113 - f1: 0.8113 - loss: 0.4240 - precision: 0.8113 - recall: 0.8113 - val_accuracy: 0.7446 - val_f1: 0.7447 - val_loss: 0.5336 - val_precision: 0.7447 - val_recall: 0.7447\n",
      "Epoch 3/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.8766 - f1: 0.8766 - loss: 0.2869 - precision: 0.8766 - recall: 0.8766 - val_accuracy: 0.7268 - val_f1: 0.7267 - val_loss: 0.6343 - val_precision: 0.7267 - val_recall: 0.7267\n",
      "Epoch 4/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 55ms/step - accuracy: 0.9107 - f1: 0.9107 - loss: 0.2150 - precision: 0.9107 - recall: 0.9107 - val_accuracy: 0.7188 - val_f1: 0.7190 - val_loss: 0.7801 - val_precision: 0.7190 - val_recall: 0.7190\n",
      "Epoch 5/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 53ms/step - accuracy: 0.9346 - f1: 0.9346 - loss: 0.1639 - precision: 0.9346 - recall: 0.9346 - val_accuracy: 0.7033 - val_f1: 0.7031 - val_loss: 0.8706 - val_precision: 0.7031 - val_recall: 0.7031\n",
      "Epoch 6/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.9435 - f1: 0.9435 - loss: 0.1341 - precision: 0.9435 - recall: 0.9435 - val_accuracy: 0.6973 - val_f1: 0.6972 - val_loss: 1.0506 - val_precision: 0.6972 - val_recall: 0.6972\n",
      "Epoch 7/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.9453 - f1: 0.9453 - loss: 0.1218 - precision: 0.9453 - recall: 0.9453 - val_accuracy: 0.6949 - val_f1: 0.6952 - val_loss: 1.3158 - val_precision: 0.6952 - val_recall: 0.6952\n",
      "Epoch 8/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.9558 - f1: 0.9558 - loss: 0.0988 - precision: 0.9558 - recall: 0.9558 - val_accuracy: 0.6917 - val_f1: 0.6919 - val_loss: 1.5341 - val_precision: 0.6919 - val_recall: 0.6919\n",
      "Epoch 9/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 52ms/step - accuracy: 0.9564 - f1: 0.9564 - loss: 0.0901 - precision: 0.9564 - recall: 0.9564 - val_accuracy: 0.6821 - val_f1: 0.6828 - val_loss: 1.6843 - val_precision: 0.6828 - val_recall: 0.6828\n",
      "Epoch 10/10\n",
      "\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 56ms/step - accuracy: 0.9593 - f1: 0.9593 - loss: 0.0877 - precision: 0.9593 - recall: 0.9593 - val_accuracy: 0.6926 - val_f1: 0.6928 - val_loss: 1.5475 - val_precision: 0.6928 - val_recall: 0.6928\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 64,\n",
    "    epochs=10,\n",
    "    validation_data=(X_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e67117",
   "metadata": {},
   "source": [
    "# 6.1| PrediÃ§Ã£o CompetiÃ§Ã£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c443aaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download do conjunto de teste a ser predito na competiÃ§Ã£o\n",
    "df_teste = pd.read_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - AnÃ¡lise de Toxidade\\\\test (4).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "000e1624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando o pipeline de tratamento de dados\n",
    "df_teste['text'] = df_teste['text'].apply(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd2cb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a coluna com os textos a serem classificados em 'array'\n",
    "X_pred = df_teste['text']\n",
    "\n",
    "# Tokenizando os textos\n",
    "X_pred = tokenizer.texts_to_sequences(X_pred)\n",
    "\n",
    "X_pred = pad_sequences(X_pred, maxlen = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c18afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m132/132\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# PrediÃ§Ã£o\n",
    "y_pred = model.predict(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "644e7822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9126071e-01, 8.7393140e-03],\n",
       "       [9.9899036e-01, 1.0096114e-03],\n",
       "       [5.8373843e-07, 9.9999940e-01],\n",
       "       ...,\n",
       "       [1.0000000e+00, 5.7883054e-09],\n",
       "       [3.6541006e-01, 6.3458997e-01],\n",
       "       [9.9125779e-01, 8.7422552e-03]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VisualizaÃ§Ã£o das classificaÃ§Ãµes feitas\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2397df",
   "metadata": {},
   "source": [
    "> Nota-se que a prediÃ§Ã£o Ã© a probabilidade do texto avaliado ser classificado para cada classe (coluna 0 = nÃ£o tÃ³xico e coluna 1 = tÃ³xico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94b784ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limiar para decidir a classe\n",
    "threshold = 0.7\n",
    "\n",
    "# Convertendo as probabilidades para as classes finais usando um limiar\n",
    "predictions = np.where(y_pred >= threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce4429c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo as classificaÃ§Ãµes em uma array\n",
    "predicted_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f8ae309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando as classificaÃ§Ãµes em uma nova coluna\n",
    "df_teste['label'] = predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4221d98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Excluindo a coluna de 'text' para manter o formato de submissÃ£o da competiÃ§Ã£o no Kaggle\n",
    "df_teste.drop('text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed37740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o df_teste em um csv para ser submetido ao Kaggle\n",
    "df_teste.to_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - AnÃ¡lise de Toxidade\\\\SubmissÃµes\\\\submissao_lstm.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1747594f",
   "metadata": {},
   "source": [
    "> O modelo de LSTM obteve um resultado de xxx de acurÃ¡cia nos dados da competiÃ§Ã£o. Com isso, outros modelos serÃ£o tambÃ©m treinados e testados para tentar alcanÃ§ar a melhor pontuaÃ§Ã£o."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a5fb8",
   "metadata": {},
   "source": [
    "# 7 | Modelo SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f84dbdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AcurÃ¡cia do modelo: 0.743452380952381\n"
     ]
    }
   ],
   "source": [
    "# DivisÃ£o dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_modeling['text_adjusted'], df_modeling['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# VetorizaÃ§Ã£o do texto usando TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Treinamento do modelo SVM\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train_vect, y_train)\n",
    "\n",
    "# Fazendo previsÃµes\n",
    "predictions = model.predict(X_test_vect)\n",
    "\n",
    "# Avaliando o modelo\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"AcurÃ¡cia do modelo:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c842b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DivisÃ£o dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_modeling['text_adjusted'], df_modeling['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# Instanciando o tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Ajustando o tokenizer nos dados de treino\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Aplicando o tokenizer aos dados de treino e teste\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Quantidade de palavras (serÃ¡ usada na camada de embedding)\n",
    "qtd_palavras = len(tokenizer.word_index) + 1\n",
    "\n",
    "## Padding -> to uniform the datas\n",
    "max_length = max(len(seq) for seq in X_train)\n",
    "\n",
    "# to test an outlier case (if one of the test dataset has longer length)\n",
    "for x in X_test:\n",
    "    if len(x) > max_length:\n",
    "        print(f\"an outlier detected: {x}\")\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = max_length)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length)\n",
    "\n",
    "# create hot_labels (idk whty tapi ini penting, kalo ga bakal error)\n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "\n",
    "# Convertendo as classificaÃ§Ãµes em uma array\n",
    "y_test_array = np.argmax(y_test, axis=1)\n",
    "y_train_array = np.argmax(y_train, axis=1)\n",
    "\n",
    "# Treinamento do modelo SVM\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train_array)\n",
    "\n",
    "# Fazendo previsÃµes\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Avaliando o modelo\n",
    "accuracy = accuracy_score(y_test_array, predictions)\n",
    "print(\"AcurÃ¡cia do modelo:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4da026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo as classificaÃ§Ãµes em uma array\n",
    "teste = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075e39da",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
