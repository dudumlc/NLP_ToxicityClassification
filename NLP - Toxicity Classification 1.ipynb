{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a530620f",
   "metadata": {},
   "source": [
    "## CONFIGURA√á√ÉO DO AMBIENTE E IMPORT DAS BIBLIOTECAS NECESS√ÅRIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c509bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dfd9f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dudu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dudu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dudu_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "import re\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba52d94",
   "metadata": {},
   "source": [
    "## DOWNLOAD DO DATASET A SER TRABALHADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfccbc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANDO O DATASET \n",
    "\n",
    "df_raw = pd.read_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - An√°lise de Toxidade\\\\train (2).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "044d4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRIANDO UM BACKUP\n",
    "\n",
    "df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef02858",
   "metadata": {},
   "source": [
    "## DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65121097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rt @user olha quem chegouuuuu, nossos queridin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>veio umas teorias muito loucas na minha cabe√ßa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@user @user üòÇüòÇüòÇüòÇmais nao tinha falado ontem qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt @user quer ser filha da puta logo comigo qu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vai besta üòÇüòÇüòÇüòÇ casquei com a ultima foto</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rt @user @user n√£o sei oq √© mais chocante, um ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@user pois √© mano que coisa chata da porra</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rt @user eu odeio sentir √≥dio de algu√©m pq eu ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>se eu ganhar raspo minha cabe√ßa\\n https://t.co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rt @user @user vlw mano tmj ÈâÇ„ÇêÁÖâÓÅèÁÖÜÔøΩ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  rt @user olha quem chegouuuuu, nossos queridin...      0\n",
       "1  veio umas teorias muito loucas na minha cabe√ßa...      1\n",
       "2  @user @user üòÇüòÇüòÇüòÇmais nao tinha falado ontem qu...      0\n",
       "3  rt @user quer ser filha da puta logo comigo qu...      1\n",
       "4           vai besta üòÇüòÇüòÇüòÇ casquei com a ultima foto      1\n",
       "5  rt @user @user n√£o sei oq √© mais chocante, um ...      1\n",
       "6         @user pois √© mano que coisa chata da porra      0\n",
       "7  rt @user eu odeio sentir √≥dio de algu√©m pq eu ...      1\n",
       "8  se eu ganhar raspo minha cabe√ßa\\n https://t.co...      0\n",
       "9                 rt @user @user vlw mano tmj ÈâÇ„ÇêÁÖâÓÅèÁÖÜÔøΩ      0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PR√â-VISUALIZA√á√ÉO DOS DADOS\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0024076",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset possui 16800 linhas e 2 colunas\n"
     ]
    }
   ],
   "source": [
    "# DIMENS√ïES DO DATASET\n",
    "\n",
    "print(\"O dataset possui\",df.shape[0],\"linhas e\",df.shape[1],\"colunas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4ca0bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9425</td>\n",
       "      <td>56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7375</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count     %\n",
       "label             \n",
       "0       9425  56.0\n",
       "1       7375  44.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AN√ÅLISE DA PROPOR√á√ÉO DE CADA UMA DAS CLASSES\n",
    "\n",
    "proporcao = df['label'].value_counts().to_frame()\n",
    "\n",
    "proporcao['%'] = (proporcao['count'] / df.shape[0]) * 100\n",
    "\n",
    "proporcao['%'] = proporcao['%'].round()\n",
    "\n",
    "proporcao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6930f70",
   "metadata": {},
   "source": [
    "## DATA CLEANING AND DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d33b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o de uma coluna com os tratamentos\n",
    "\n",
    "df['text_adjusted'] = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb2c3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alterando todos os caracteres para min√∫sculos\n",
    "\n",
    "df['text_adjusted'] = df['text_adjusted'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a166eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirada do termo \"@user\" - \"rt\" - \"\\n\", pois referenciam apenas o ato de retweetar, o perfil do tweet e quebras de linhas\n",
    "\n",
    "df['text_adjusted'] = df['text_adjusted'].str.replace(\"@user\",\"\")\n",
    "\n",
    "df['text_adjusted'] = df['text_adjusted'].str.replace(\"rt \",\"\")\n",
    "\n",
    "df['text_adjusted'] = df['text_adjusted'].str.replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32fbab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirada de todas as pontua√ß√µes dos textos\n",
    "\n",
    "for i in string.punctuation:\n",
    "    df['text_adjusted'] = df['text_adjusted'].str.replace(i, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3d0ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirada de links encurtados, todos come√ßam com https \n",
    "\n",
    "def remove_https(text):\n",
    "    return re.sub(r'https\\S+', '', text)\n",
    "\n",
    "df['text_adjusted'] = df['text_adjusted'].apply(remove_https)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81abe068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remover emojis de um texto\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # s√≠mbolos e pictogramas\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transporte e s√≠mbolos mapas\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # bandeiras (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # caracteres chineses comuns\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # s√≠mbolo variante de texto\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Retirando emojis do texto \n",
    "df['text_adjusted'] = df['text_adjusted'].apply(remove_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab84a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirada de acentos \n",
    "\n",
    "for i,v in enumerate(df['text_adjusted']):\n",
    "    df['text_adjusted'][i] = unidecode.unidecode(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1dfa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo as stopwords em ptbr\n",
    "stop_words = set(stopwords.words('portuguese'))  # Altere para o idioma desejado\n",
    "\n",
    "# Retirada de stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "df['text_adjusted'] = df['text_adjusted'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3cc8e5",
   "metadata": {},
   "source": [
    "## MODELAGEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b4edb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um indice para termo utilizado nos tweets\n",
    "\n",
    "termos = {}\n",
    "id = 0\n",
    "for i in df['text_adjusted']:\n",
    "    for v in i.split():\n",
    "        termos[id] = v\n",
    "        id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c7f2c",
   "metadata": {},
   "source": [
    "## Treinamento SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbe3e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = df[['text_adjusted','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "acd3d3d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia do modelo: 0.7404761904761905\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Divis√£o dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_treino['text_adjusted'], df_treino['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vetoriza√ß√£o do texto usando TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Treinamento do modelo SVM\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train_vect, y_train)\n",
    "\n",
    "# Fazendo previs√µes\n",
    "predictions = model.predict(X_test_vect)\n",
    "\n",
    "# Avaliando o modelo\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Acur√°cia do modelo:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532f3fb1",
   "metadata": {},
   "source": [
    "## Treinamento SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13ebc6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = df[['text_adjusted','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d16bfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia do modelo: 0.6705357142857142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Divis√£o dos dados em conjunto de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_treino['text_adjusted'], df_treino['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vetoriza√ß√£o do texto usando TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# Treinamento do modelo \n",
    "clf = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "clf.fit(X_train_vect, y_train)\n",
    "\n",
    "# Fazendo previs√µes\n",
    "predictions_clf = clf.predict(X_test_vect)\n",
    "\n",
    "# Avaliando o modelo\n",
    "accuracy_clf = accuracy_score(y_test, predictions_clf)\n",
    "print(\"Acur√°cia do modelo:\", accuracy_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bf9dd",
   "metadata": {},
   "source": [
    "## Teste SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c546f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste\n",
    "\n",
    "df_teste = pd.read_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - An√°lise de Toxidade\\\\test (4).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77c88e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando apenas a coluna com os tweets\n",
    "\n",
    "x_test_svc = df_teste[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17e2e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o de um pipeline de tratamento dos dados de teste (os mesmos que foram feitos para os de treino)\n",
    "\n",
    "def pipeline(text):\n",
    "    text = text.str.lower() # tudo minusculo\n",
    "    \n",
    "    text = text.str.replace(\"@user\",\"\")\n",
    "    text = text.str.replace(\"rt \",\"\") # sem termos desnecess√°rios\n",
    "    text = text.str.replace(\"\\n\",\"\")\n",
    "    \n",
    "    for i in string.punctuation:\n",
    "        text = text.str.replace(i, \"\") # sem pontua√ß√£o\n",
    "    \n",
    "    text = text.apply(remove_https) # sem links\n",
    "    \n",
    "    text = text.apply(remove_emojis) # sem emojis\n",
    "    \n",
    "    for i,v in enumerate(text):\n",
    "        text[i] = unidecode.unidecode(v) # normalizados (sem acento)\n",
    "    \n",
    "    text = text.apply(remove_stopwords) # sem stopwords\n",
    "\n",
    "    return text\n",
    "\n",
    "# Aplicando o pipeline na coluna de test\n",
    "x_test_svc = x_test_svc.apply(pipeline)\n",
    "\n",
    "# Transformando em array\n",
    "x_test_svc = x_test_svc['text']\n",
    "\n",
    "# Vetorizando \n",
    "x_test_svc_vect = vectorizer.transform(x_test_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40dbe948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predi√ß√£o\n",
    "\n",
    "predictions_svc = model.predict(x_test_svc_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "142abf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando coluna com as predi√ß√µes\n",
    "df_teste['label'] = predictions_svc\n",
    "\n",
    "# Formatando para o modo de submiss√£o\n",
    "df_teste = df_teste.drop('text', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6920cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.to_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - An√°lise de Toxidade\\\\submissao_svc2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850efd1c",
   "metadata": {},
   "source": [
    "## Teste SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed55c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste\n",
    "\n",
    "df_teste = pd.read_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - An√°lise de Toxidade\\\\test (4).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c7c7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando apenas a coluna com os tweets\n",
    "\n",
    "x_test_clf = df_teste[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a63018e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria√ß√£o de um pipeline de tratamento dos dados de teste (os mesmos que foram feitos para os de treino)\n",
    "\n",
    "def pipeline(text):\n",
    "    text = text.str.lower() # tudo minusculo\n",
    "    \n",
    "    text = text.str.replace(\"@user\",\"\")\n",
    "    text = text.str.replace(\"rt \",\"\") # sem termos desnecess√°rios\n",
    "    text = text.str.replace(\"\\n\",\"\")\n",
    "    \n",
    "    for i in string.punctuation:\n",
    "        text = text.str.replace(i, \"\") # sem pontua√ß√£o\n",
    "    \n",
    "    text = text.apply(remove_https) # sem links\n",
    "    \n",
    "    text = text.apply(remove_emojis) # sem emojis\n",
    "    \n",
    "    for i,v in enumerate(text):\n",
    "        text[i] = unidecode.unidecode(v) # normalizados (sem acento)\n",
    "    \n",
    "    text = text.apply(remove_stopwords) # sem stopwords\n",
    "\n",
    "    return text\n",
    "\n",
    "# Aplicando o pipeline na coluna de test\n",
    "x_test_clf = x_test_clf.apply(pipeline)\n",
    "\n",
    "# Transformando em array\n",
    "x_test_clf = x_test_clf['text']\n",
    "\n",
    "# Vetorizando \n",
    "x_test_clf_vect = vectorizer.transform(x_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baf2ecbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predi√ß√£o\n",
    "\n",
    "predictions_clf = clf.predict(x_test_clf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3f1ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando coluna com as predi√ß√µes\n",
    "df_teste['label'] = predictions_clf\n",
    "\n",
    "# Formatando para o modo de submiss√£o\n",
    "df_teste = df_teste.drop('text', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef41b236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste.to_csv(\"C:\\\\Users\\\\Dudu_\\\\OneDrive\\\\Documentos\\\\Estudos\\\\10. NLP - An√°lise de Toxidade\\\\submissao_sgdclassifier.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
